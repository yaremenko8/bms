{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"efficientnet-multi-layer-lstm-training.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Yzl-AVPT9ZUG"},"source":["# SUMMARY\n","\n","This notebook builds on the great pipeline [introduced](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter) by Y. Nakama and [adapted to EfficientNets](https://www.kaggle.com/konradb/model-train-efficientnet) by Konrad Banachewicz. The notebook further extendens the pipeline by adding support for multi-layer LSTM in the decoder part. Most of the code changes are concentrated in the model class. Please credit the original authors for their contributions.\n","\n","This is training notebook. Inference with multi-layer LSTM decoder is demonstarted [in this notebook](https://www.kaggle.com/kozodoi/efficientnet-multi-layer-lstm-inference).\n","\n","### References:\n","\n","- [starter notebook from Y. Nakama](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n","- [adapted notebook from Konrad](https://www.kaggle.com/yasufuminakama/inchi-resnet-lstm-with-attention-starter)\n","- [PyTorch tutorial on image captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)\n","- [two-layer RNN implementation](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/pull/79)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzpw83pU9e-0","executionInfo":{"status":"ok","timestamp":1621339601562,"user_tz":-120,"elapsed":23372,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"6b5799f9-ed6a-4237-bdca-5abb5b914fa6"},"source":["from google.colab import drive\n","\n","\n","drive.mount('/content/drive/')\n","rpath = '/content/drive/My Drive/cheminformatics/bms'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQfIhGqp-MSV","executionInfo":{"status":"ok","timestamp":1621339608044,"user_tz":-120,"elapsed":29847,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"eb67c857-8706-462a-d913-f64b1a3ba9ab"},"source":["!pip install levenshtein albumentations==0.4.6 timm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting levenshtein\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/41/ff25ae28c972a63abde29cd5cea7c648ae0e16b334693cede0522e66dd68/levenshtein-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (158kB)\n","\r\u001b[K     |██                              | 10kB 974kB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 133kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 6.1MB/s \n","\u001b[?25hCollecting albumentations==0.4.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n","\u001b[K     |████████████████████████████████| 122kB 34.9MB/s \n","\u001b[?25hCollecting timm\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/89/d94f59780b5dd973154bf506d8ce598f6bfe7cc44dd445d644d6d3be8c39/timm-0.4.5-py3-none-any.whl (287kB)\n","\u001b[K     |████████████████████████████████| 296kB 36.8MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from levenshtein) (56.1.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Collecting imgaug>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n","\u001b[K     |████████████████████████████████| 952kB 22.4MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5.1)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.6-cp37-none-any.whl size=65163 sha256=3c1cd136c4b10ebfcdfee404a870d00e0777dc8fa48fe0d9a941dab03033e698\n","  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n","Successfully built albumentations\n","Installing collected packages: levenshtein, imgaug, albumentations, timm\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.6 imgaug-0.4.0 levenshtein-0.12.0 timm-0.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MMVriscD__Jr","executionInfo":{"status":"ok","timestamp":1621339629706,"user_tz":-120,"elapsed":51504,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"ec79bb20-d67c-45c5-dcf1-018358e7e429"},"source":["!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting cloud-tpu-client==0.10\n","  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n","Collecting torch-xla==1.8.1\n","\u001b[?25l  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl (145.0MB)\n","\u001b[K     |████████████████████████████████| 145.0MB 91kB/s \n","\u001b[?25hCollecting google-api-python-client==1.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.30.0)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (56.1.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (20.9)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n","\u001b[31mERROR: earthengine-api 0.1.264 has requirement google-api-python-client<2,>=1.12.1, but you'll have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n","Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla\n","  Found existing installation: google-api-python-client 1.12.8\n","    Uninstalling google-api-python-client-1.12.8:\n","      Successfully uninstalled google-api-python-client-1.12.8\n","Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"papermill":{"duration":5.105933,"end_time":"2021-04-07T08:27:02.380708","exception":false,"start_time":"2021-04-07T08:26:57.274775","status":"completed"},"tags":[],"id":"OLW54LUH9ZUN","executionInfo":{"status":"ok","timestamp":1621339637844,"user_tz":-120,"elapsed":59594,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["import os\n","import gc \n","from matplotlib import pyplot as plt\n","import zipfile\n","# import torch_xla\n","# import torch_xla.core.xla_model as xm\n","from skimage import io, morphology, img_as_bool, segmentation\n","from scipy import ndimage as ndi\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import random\n","import os\n","\n","OUTPUT_DIR = './'\n","if not os.path.exists(OUTPUT_DIR):\n","    os.makedirs(OUTPUT_DIR)\n","    \n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","import sys\n","# sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n","if rpath not in sys.path:\n","    sys.path.append(rpath)\n","\n","from preprocess import get_image, clean_image, get_letters, erode_image\n","\n","import os\n","import gc\n","import re\n","import math\n","import time\n","import random\n","import shutil\n","import pickle\n","from pathlib import Path\n","from contextlib import contextmanager\n","from collections import defaultdict, Counter\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import Levenshtein\n","from sklearn import preprocessing\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","from functools import partial\n","\n","import cv2\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD\n","import torchvision.models as models\n","from torch.nn.parameter import Parameter\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n","\n","from albumentations import (\n","    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n","    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n","    IAAAdditiveGaussianNoise, Transpose, Blur\n","    )\n","from albumentations.pytorch import ToTensorV2\n","from albumentations import ImageOnlyTransform\n","\n","import timm\n","\n","import warnings \n","warnings.filterwarnings('ignore')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBp0ad7r7SoK","executionInfo":{"status":"ok","timestamp":1621339698711,"user_tz":-120,"elapsed":120442,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"36f5a0f3-3a40-402f-cd4c-7beda493953b"},"source":["trainzip = zipfile.ZipFile(os.path.join(rpath, 'data/train.zip'))\n","print(\"train cached\")\n","testzip = zipfile.ZipFile(os.path.join(rpath, 'data/test.zip'))\n","print(\"test cached\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["train cached\n","test cached\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1W0l3HT_teu","executionInfo":{"status":"ok","timestamp":1621340268258,"user_tz":-120,"elapsed":1199,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"0076a212-fd4f-43a6-cfc6-e14ec6e407a5"},"source":["# device = xm.xla_device()\n","device"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"VstNYhZM9ZUO"},"source":["CFG class now includes a new parameter: `decoder_layers`. For illustration purposes, I am running a two-layer LSTM for 1 epoch on 100k images."]},{"cell_type":"code","metadata":{"papermill":{"duration":0.022485,"end_time":"2021-04-07T08:27:10.713561","exception":false,"start_time":"2021-04-07T08:27:10.691076","status":"completed"},"tags":[],"id":"sJOJU1Hp9ZUP","executionInfo":{"status":"ok","timestamp":1621340268881,"user_tz":-120,"elapsed":1806,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["#  n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n","#   'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n","#   'efficientnet-b6': 2304, 'efficientnet-b7': 2560}\n","\n","# This is not, to put it mildly, the most elegant solution ever - but I ran into some trouble \n","# with checking the size of feature spaces programmatically inside the CFG definition.\n","\n","class CFG:\n","    debug          = True\n","    apex           = False\n","    max_len        = 275\n","    print_freq     = 250\n","    num_workers    = 0\n","    model_name     = 'efficientnet_b2'\n","    enc_size       = 1408\n","    samp_size      = 100000\n","    size           = 288\n","    scheduler      = 'CosineAnnealingLR' \n","    epochs         = 1 \n","    T_max          = 4  \n","    encoder_lr     = 1e-4\n","    decoder_lr     = 4e-4\n","    min_lr         = 1e-6\n","    batch_size     = 8\n","    weight_decay   = 1e-6\n","    gradient_accumulation_steps = 1\n","    max_grad_norm  = 10\n","    attention_dim  = 256\n","    embed_dim      = 512\n","    decoder_dim    = 512\n","    decoder_layers = 2     # number of LSTM layers\n","    dropout        = 0.5\n","    seed           = 42\n","    n_fold         = 5\n","    trn_fold       = 0 \n","    train          = True\n","    train_path     = rpath+'/data/'\n","    prep_path      = rpath+'/preprocessed/'\n","    prev_model     = rpath+'/models/muh_best.pth'"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"aMnfor02-05X","executionInfo":{"status":"ok","timestamp":1621340268883,"user_tz":-120,"elapsed":1799,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"a3100077-2c9f-4542-bb91-9e1631a4fabb"},"source":["rpath"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/cheminformatics/bms'"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011683,"end_time":"2021-04-07T08:27:10.737552","exception":false,"start_time":"2021-04-07T08:27:10.725869","status":"completed"},"tags":[],"id":"dnGldUAr9ZUP"},"source":["# Functions"]},{"cell_type":"code","metadata":{"papermill":{"duration":0.037989,"end_time":"2021-04-07T08:27:10.787242","exception":false,"start_time":"2021-04-07T08:27:10.749253","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"-A0HAqDZ9ZUQ","executionInfo":{"status":"ok","timestamp":1621340269302,"user_tz":-120,"elapsed":2203,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"4486cb4e-50df-42ab-9d54-91b843d7d5b6"},"source":["class Tokenizer(object):\n","    \n","    def __init__(self):\n","        self.stoi = {}\n","        self.itos = {}\n","\n","    def __len__(self):\n","        return len(self.stoi)\n","    \n","    def fit_on_texts(self, texts):\n","        vocab = set()\n","        for text in texts:\n","            vocab.update(text.split(' '))\n","        vocab = sorted(vocab)\n","        vocab.append('<sos>')\n","        vocab.append('<eos>')\n","        vocab.append('<pad>')\n","        for i, s in enumerate(vocab):\n","            self.stoi[s] = i\n","        self.itos = {item[1]: item[0] for item in self.stoi.items()}\n","        \n","    def text_to_sequence(self, text):\n","        sequence = []\n","        sequence.append(self.stoi['<sos>'])\n","        for s in text.split(' '):\n","            sequence.append(self.stoi[s])\n","        sequence.append(self.stoi['<eos>'])\n","        return sequence\n","    \n","    def texts_to_sequences(self, texts):\n","        sequences = []\n","        for text in texts:\n","            sequence = self.text_to_sequence(text)\n","            sequences.append(sequence)\n","        return sequences\n","\n","    def sequence_to_text(self, sequence):\n","        return ''.join(list(map(lambda i: self.itos[i], sequence)))\n","    \n","    def sequences_to_texts(self, sequences):\n","        texts = []\n","        for sequence in sequences:\n","            text = self.sequence_to_text(sequence)\n","            texts.append(text)\n","        return texts\n","    \n","    def predict_caption(self, sequence):\n","        caption = ''\n","        for i in sequence:\n","            if i == self.stoi['<eos>'] or i == self.stoi['<pad>']:\n","                break\n","            caption += self.itos[i]\n","        return caption\n","    \n","    def predict_captions(self, sequences):\n","        captions = []\n","        for sequence in sequences:\n","            caption = self.predict_caption(sequence)\n","            captions.append(caption)\n","        return captions\n","\n","tokenizer = torch.load(CFG.prep_path + 'tokenizer2.pth')\n","print(f\"tokenizer.stoi: {tokenizer.stoi}\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["tokenizer.stoi: {'(': 0, ')': 1, '+': 2, ',': 3, '-': 4, '/b': 5, '/c': 6, '/h': 7, '/i': 8, '/m': 9, '/s': 10, '/t': 11, '0': 12, '1': 13, '10': 14, '100': 15, '101': 16, '102': 17, '103': 18, '104': 19, '105': 20, '106': 21, '107': 22, '108': 23, '109': 24, '11': 25, '110': 26, '111': 27, '112': 28, '113': 29, '114': 30, '115': 31, '116': 32, '117': 33, '118': 34, '119': 35, '12': 36, '120': 37, '121': 38, '122': 39, '123': 40, '124': 41, '125': 42, '126': 43, '127': 44, '128': 45, '129': 46, '13': 47, '130': 48, '131': 49, '132': 50, '133': 51, '134': 52, '135': 53, '136': 54, '137': 55, '138': 56, '139': 57, '14': 58, '140': 59, '141': 60, '142': 61, '143': 62, '144': 63, '145': 64, '146': 65, '147': 66, '148': 67, '149': 68, '15': 69, '150': 70, '151': 71, '152': 72, '153': 73, '154': 74, '155': 75, '156': 76, '157': 77, '158': 78, '159': 79, '16': 80, '161': 81, '163': 82, '165': 83, '167': 84, '17': 85, '18': 86, '19': 87, '2': 88, '20': 89, '21': 90, '22': 91, '23': 92, '24': 93, '25': 94, '26': 95, '27': 96, '28': 97, '29': 98, '3': 99, '30': 100, '31': 101, '32': 102, '33': 103, '34': 104, '35': 105, '36': 106, '37': 107, '38': 108, '39': 109, '4': 110, '40': 111, '41': 112, '42': 113, '43': 114, '44': 115, '45': 116, '46': 117, '47': 118, '48': 119, '49': 120, '5': 121, '50': 122, '51': 123, '52': 124, '53': 125, '54': 126, '55': 127, '56': 128, '57': 129, '58': 130, '59': 131, '6': 132, '60': 133, '61': 134, '62': 135, '63': 136, '64': 137, '65': 138, '66': 139, '67': 140, '68': 141, '69': 142, '7': 143, '70': 144, '71': 145, '72': 146, '73': 147, '74': 148, '75': 149, '76': 150, '77': 151, '78': 152, '79': 153, '8': 154, '80': 155, '81': 156, '82': 157, '83': 158, '84': 159, '85': 160, '86': 161, '87': 162, '88': 163, '89': 164, '9': 165, '90': 166, '91': 167, '92': 168, '93': 169, '94': 170, '95': 171, '96': 172, '97': 173, '98': 174, '99': 175, 'B': 176, 'Br': 177, 'C': 178, 'Cl': 179, 'D': 180, 'F': 181, 'H': 182, 'I': 183, 'N': 184, 'O': 185, 'P': 186, 'S': 187, 'Si': 188, 'T': 189, '<sos>': 190, '<eos>': 191, '<pad>': 192}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"papermill":{"duration":0.027729,"end_time":"2021-04-07T08:27:10.827442","exception":false,"start_time":"2021-04-07T08:27:10.799713","status":"completed"},"tags":[],"id":"jOLyp-n39ZUR","executionInfo":{"status":"ok","timestamp":1621340269303,"user_tz":-120,"elapsed":2192,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["def get_score(y_true, y_pred):\n","    scores = []\n","    for true, pred in zip(y_true, y_pred):\n","        score = Levenshtein.distance(true, pred)\n","        scores.append(score)\n","    avg_score = np.mean(scores)\n","    return avg_score\n","\n","\n","def init_logger(log_file=OUTPUT_DIR+'train.log'):\n","    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=log_file)\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = init_logger()\n","\n","\n","def seed_torch(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_torch(seed = CFG.seed)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.021152,"end_time":"2021-04-07T08:27:10.902922","exception":false,"start_time":"2021-04-07T08:27:10.88177","status":"completed"},"tags":[],"id":"OsvcuezK9ZUS","executionInfo":{"status":"ok","timestamp":1621340269304,"user_tz":-120,"elapsed":2177,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["def bms_collate(batch):\n","    imgs, labels, label_lengths = [], [], []\n","    for data_point in batch:\n","        imgs.append(data_point[0])\n","        labels.append(data_point[1])\n","        label_lengths.append(data_point[2])\n","    labels = pad_sequence(labels, batch_first = True, padding_value = tokenizer.stoi[\"<pad>\"])\n","    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.021209,"end_time":"2021-04-07T08:27:10.936685","exception":false,"start_time":"2021-04-07T08:27:10.915476","status":"completed"},"tags":[],"id":"nxwAXKG-9ZUT","executionInfo":{"status":"ok","timestamp":1621340269304,"user_tz":-120,"elapsed":2171,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["####### CNN ENCODER\n","\n","class Encoder(nn.Module):\n","    def __init__(self, model_name = CFG.model_name, pretrained = False):\n","        super().__init__()\n","        self.cnn = timm.create_model(model_name, pretrained = pretrained)\n","\n","    def forward(self, x):\n","        bs       = x.size(0)\n","        features = self.cnn.forward_features(x)\n","        features = features.permute(0, 2, 3, 1)\n","        return features"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hil6R-gJ9ZUT"},"source":["The class `DecoderWithAttention` is updated to support a multi-layer LSTM."]},{"cell_type":"code","metadata":{"papermill":{"duration":0.067717,"end_time":"2021-04-07T08:27:11.017304","exception":false,"start_time":"2021-04-07T08:27:10.949587","status":"completed"},"tags":[],"id":"TaxX1sPP9ZUT","executionInfo":{"status":"ok","timestamp":1621340269305,"user_tz":-120,"elapsed":2164,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["####### RNN DECODER\n","\n","# attention module\n","class Attention(nn.Module):\n","    '''\n","    Attention network for calculate attention value\n","    '''\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        '''\n","        :param encoder_dim: input size of encoder network\n","        :param decoder_dim: input size of decoder network\n","        :param attention_dim: input size of attention network\n","        '''\n","        super(Attention, self).__init__()\n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n","        self.full_att    = nn.Linear(attention_dim, 1)            # linear layer to calculate values to be softmax-ed\n","        self.relu        = nn.ReLU()\n","        self.softmax     = nn.Softmax(dim = 1)  # softmax layer to calculate weights\n","\n","    def forward(self, encoder_out, decoder_hidden):\n","        att1  = self.encoder_att(encoder_out)     # (batch_size, num_pixels, attention_dim)\n","        att2  = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n","        att   = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n","        alpha = self.softmax(att)                 # (batch_size, num_pixels)\n","        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim = 1)  # (batch_size, encoder_dim)\n","        return attention_weighted_encoding, alpha\n","    \n","    \n","# custom LSTM cell\n","def LSTMCell(input_size, hidden_size, **kwargs):\n","    m = nn.LSTMCell(input_size, hidden_size, **kwargs)\n","    for name, param in m.named_parameters():\n","        if 'weight' in name or 'bias' in name:\n","            param.data.uniform_(-0.1, 0.1)\n","    return m\n","\n","\n","# decoder\n","class DecoderWithAttention(nn.Module):\n","    '''\n","    Decoder network with attention network used for training\n","    '''\n","\n","    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim, dropout, num_layers):\n","        '''\n","        :param attention_dim: input size of attention network\n","        :param embed_dim: input size of embedding network\n","        :param decoder_dim: input size of decoder network\n","        :param vocab_size: total number of characters used in training\n","        :param encoder_dim: input size of encoder network\n","        :param num_layers: number of the LSTM layers\n","        :param dropout: dropout rate\n","        '''\n","        super(DecoderWithAttention, self).__init__()\n","        self.encoder_dim   = encoder_dim\n","        self.attention_dim = attention_dim\n","        self.embed_dim     = embed_dim\n","        self.decoder_dim   = decoder_dim\n","        self.vocab_size    = vocab_size\n","        self.dropout       = dropout\n","        self.num_layers    = num_layers\n","        self.device        = device\n","        self.attention     = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n","        self.embedding     = nn.Embedding(vocab_size, embed_dim)                 # embedding layer\n","        self.dropout       = nn.Dropout(p = self.dropout)\n","        self.decode_step   = nn.ModuleList([LSTMCell(embed_dim + encoder_dim if layer == 0 else embed_dim, embed_dim) for layer in range(self.num_layers)]) # decoding LSTMCell        \n","        self.init_h        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n","        self.init_c        = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n","        self.f_beta        = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n","        self.sigmoid       = nn.Sigmoid()\n","        self.fc            = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n","        self.init_weights()                                      # initialize some layers with the uniform distribution\n","\n","    def init_weights(self):\n","        self.embedding.weight.data.uniform_(-0.1, 0.1)\n","        self.fc.bias.data.fill_(0)\n","        self.fc.weight.data.uniform_(-0.1, 0.1)\n","\n","    def load_pretrained_embeddings(self, embeddings):\n","        self.embedding.weight = nn.Parameter(embeddings)\n","\n","    def fine_tune_embeddings(self, fine_tune = True):\n","        for p in self.embedding.parameters():\n","            p.requires_grad = fine_tune\n","\n","    def init_hidden_state(self, encoder_out):\n","        mean_encoder_out = encoder_out.mean(dim = 1)\n","        h = [self.init_h(mean_encoder_out) for i in range(self.num_layers)]  # (batch_size, decoder_dim)\n","        c = [self.init_c(mean_encoder_out) for i in range(self.num_layers)]\n","        return h, c\n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        '''\n","        :param encoder_out: output of encoder network\n","        :param encoded_captions: transformed sequence from character to integer\n","        :param caption_lengths: length of transformed sequence\n","        '''\n","        batch_size       = encoder_out.size(0)\n","        encoder_dim      = encoder_out.size(-1)\n","        vocab_size       = self.vocab_size\n","        encoder_out      = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels       = encoder_out.size(1)\n","        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim = 0, descending = True)\n","        encoder_out      = encoder_out[sort_ind]\n","        encoded_captions = encoded_captions[sort_ind]\n","        \n","        # embedding transformed sequence for vector\n","        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n","        \n","        # Initialize LSTM state, initialize cell_vector and hidden_vector\n","        prev_h, prev_c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","        \n","        # set decode length by caption length - 1 because of omitting start token\n","        decode_lengths = (caption_lengths - 1).tolist()\n","        predictions    = torch.zeros(batch_size, max(decode_lengths), vocab_size, device = self.device)\n","        alphas         = torch.zeros(batch_size, max(decode_lengths), num_pixels, device = self.device)\n","        \n","        # predict sequence\n","        for t in range(max(decode_lengths)):\n","            batch_size_t = sum([l > t for l in decode_lengths])\n","            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n","                                                                prev_h[-1][:batch_size_t])\n","            gate = self.sigmoid(self.f_beta(prev_h[-1][:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n","            attention_weighted_encoding = gate * attention_weighted_encoding\n","\n","            input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n","            \n","            for i, rnn in enumerate(self.decode_step):\n","                # recurrent cell\n","                h, c = rnn(input, (prev_h[i][:batch_size_t], prev_c[i][:batch_size_t])) # cell_vector and hidden_vector\n","\n","                # hidden state becomes the input to the next layer\n","                input = self.dropout(h)\n","\n","                # save state for next time step\n","                prev_h[i] = h\n","                prev_c[i] = c\n","                \n","            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n","            predictions[:batch_size_t, t, :] = preds\n","            alphas[:batch_size_t, t, :]      = alpha\n","            \n","        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n","    \n","    def predict(self, encoder_out, decode_lengths, tokenizer):\n","        \n","        # size variables\n","        batch_size  = encoder_out.size(0)\n","        encoder_dim = encoder_out.size(-1)\n","        vocab_size  = self.vocab_size\n","        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n","        num_pixels  = encoder_out.size(1)\n","        \n","        # embed start tocken for LSTM input\n","        start_tockens = torch.ones(batch_size, dtype = torch.long, device = self.device) * tokenizer.stoi['<sos>']\n","        embeddings    = self.embedding(start_tockens)\n","        \n","        # initialize hidden state and cell state of LSTM cell\n","        h, c        = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n","        predictions = torch.zeros(batch_size, decode_lengths, vocab_size, device = self.device)\n","        \n","        # predict sequence\n","        end_condition = torch.zeros(batch_size, dtype=torch.long, device = self.device)\n","        for t in range(decode_lengths):\n","            awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n","            gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n","            awe        = gate * awe\n","            \n","            input = torch.cat([embeddings, awe], dim=1)\n"," \n","            for j, rnn in enumerate(self.decode_step):\n","                at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n","                input = self.dropout(at_h)\n","                h[j]  = at_h\n","                c[j]  = at_c\n","            \n","            preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n","            predictions[:, t, :] = preds\n","            end_condition |= (torch.argmax(preds, -1) == tokenizer.stoi[\"<eos>\"])\n","            if end_condition.sum() == batch_size:\n","                break\n","            embeddings = self.embedding(torch.argmax(preds, -1))\n","        \n","        return predictions\n","    \n","    # beam search\n","    def forward_step(self, prev_tokens, hidden, encoder_out, function):\n","        \n","        h, c = hidden\n","        #h, c = h.squeeze(0), c.squeeze(0)\n","        h, c = [hi.squeeze(0) for hi in h], [ci.squeeze(0) for ci in c]\n","        \n","        embeddings = self.embedding(prev_tokens)\n","        if embeddings.dim() == 3:\n","            embeddings = embeddings.squeeze(1)\n","            \n","        awe, alpha = self.attention(encoder_out, h[-1])  # (s, encoder_dim), (s, num_pixels)\n","        gate       = self.sigmoid(self.f_beta(h[-1]))    # gating scalar, (s, encoder_dim)\n","        awe        = gate * awe\n","        \n","        input = torch.cat([embeddings, awe], dim = 1)\n","        for j, rnn in enumerate(self.decode_step):\n","            at_h, at_c = rnn(input, (h[j], c[j]))  # (s, decoder_dim)\n","            input = self.dropout(at_h)\n","            h[j]  = at_h\n","            c[j]  = at_c\n","\n","        preds = self.fc(self.dropout(h[-1]))  # (batch_size_t, vocab_size)\n","\n","        #hidden = (h.unsqueeze(0), c.unsqueeze(0))\n","        hidden = [hi.unsqueeze(0) for hi in h], [ci.unsqueeze(0) for ci in c]\n","        predicted_softmax = function(preds, dim = 1)\n","        \n","        return predicted_softmax, hidden, None"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.039205,"end_time":"2021-04-07T08:27:11.070219","exception":false,"start_time":"2021-04-07T08:27:11.031014","status":"completed"},"tags":[],"id":"4M7XDYWQ9ZUV","executionInfo":{"status":"ok","timestamp":1621340269881,"user_tz":-120,"elapsed":2728,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["# Helper functions\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val   = 0\n","        self.avg   = 0\n","        self.sum   = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val    = val\n","        self.sum   += val * n\n","        self.count += n\n","        self.avg    = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s   = now - since\n","    es  = s / (percent)\n","    rs  = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","\n","def train_fn(train_loader, encoder, decoder, criterion, \n","             encoder_optimizer, decoder_optimizer, epoch,\n","             encoder_scheduler, decoder_scheduler, device):\n","    \n","    batch_time = AverageMeter()\n","    data_time  = AverageMeter()\n","    losses     = AverageMeter()\n","    \n","    # switch to train mode\n","    encoder.train()\n","    decoder.train()\n","    \n","    start = end = time.time()\n","    global_step = 0\n","    \n","    for step, (images, labels, label_lengths) in enumerate(train_loader):\n","        \n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        \n","        images        = images.to(device)\n","        labels        = labels.to(device)\n","        label_lengths = label_lengths.to(device)\n","        batch_size    = images.size(0)\n","        \n","        features = encoder(images)\n","        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n","        targets     = caps_sorted[:, 1:]\n","        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n","        targets     = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n","        loss        = criterion(predictions, targets)\n","        \n","        # record loss\n","        losses.update(loss.item(), batch_size)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","            \n","        if CFG.apex:\n","            with amp.scale_loss(loss, decoder_optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","        else:\n","            loss.backward()\n","            \n","        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n","        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n","        \n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            global_step += 1\n","            \n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n","                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n","                  #'Encoder LR: {encoder_lr:.6f}  '\n","                  #'Decoder LR: {decoder_lr:.6f}  '\n","                  .format(\n","                   epoch+1, step, len(train_loader), \n","                   batch_time        = batch_time,\n","                   data_time         = data_time, \n","                   loss              = losses,\n","                   remain            = timeSince(start, float(step+1)/len(train_loader)),\n","                   encoder_grad_norm = encoder_grad_norm,\n","                   decoder_grad_norm = decoder_grad_norm,\n","                   #encoder_lr=encoder_scheduler.get_lr()[0],\n","                   #decoder_lr=decoder_scheduler.get_lr()[0],\n","                   ))\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n","    \n","    batch_time = AverageMeter()\n","    data_time  = AverageMeter()\n","    \n","    # switch to evaluation mode\n","    encoder.eval()\n","    decoder.eval()\n","    \n","    text_preds = []\n","    start = end = time.time()\n","    \n","    for step, (images) in enumerate(valid_loader):\n","        \n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","        \n","        images     = images.to(device)\n","        batch_size = images.size(0)\n","        \n","        with torch.no_grad():\n","            features    = encoder(images)\n","            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n","            \n","        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n","        _text_preds        = tokenizer.predict_captions(predicted_sequence)\n","        text_preds.append(_text_preds)\n","        \n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n","                  'Elapsed {remain:s} '\n","                  .format(\n","                   step, len(valid_loader), \n","                   batch_time = batch_time,\n","                   data_time  = data_time,\n","                   remain     = timeSince(start, float(step+1)/len(valid_loader)),\n","                   ))\n","            \n","    text_preds = np.concatenate(text_preds)\n","    return text_preds"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.038367,"end_time":"2021-04-07T08:27:11.123364","exception":false,"start_time":"2021-04-07T08:27:11.084997","status":"completed"},"tags":[],"id":"hdMyGekH9ZUV","executionInfo":{"status":"ok","timestamp":1621340269888,"user_tz":-120,"elapsed":2725,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["# ====================================================\n","# Train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    trn_idx = folds[folds['fold'] != fold].index\n","    val_idx = folds[folds['fold'] == fold].index\n","\n","    train_folds  = folds.loc[trn_idx].reset_index(drop = True)\n","    valid_folds  = folds.loc[val_idx].reset_index(drop = True)\n","    valid_labels = valid_folds['InChI'].values\n","\n","    train_dataset = TrainDataset(train_folds, tokenizer, transform = get_transforms(data = 'train'))\n","    valid_dataset = TestDataset(valid_folds, transform = get_transforms(data = 'valid'))\n","\n","    train_loader = DataLoader(train_dataset, \n","                              batch_size  = CFG.batch_size, \n","                              shuffle     = True, \n","                              num_workers = CFG.num_workers, \n","                              pin_memory  = True,\n","                              drop_last   = True, \n","                              collate_fn  = bms_collate)\n","    valid_loader = DataLoader(valid_dataset, \n","                              batch_size  = CFG.batch_size, \n","                              shuffle     = False, \n","                              num_workers = CFG.num_workers,\n","                              pin_memory  = True, \n","                              drop_last   = False)\n","    \n","    # ====================================================\n","    # scheduler \n","    # ====================================================\n","    def get_scheduler(optimizer):\n","        if CFG.scheduler=='ReduceLROnPlateau':\n","            scheduler = ReduceLROnPlateau(optimizer, \n","                                          mode     = 'min', \n","                                          factor   = CFG.factor, \n","                                          patience = CFG.patience, \n","                                          verbose  = True, \n","                                          eps      = CFG.eps)\n","        elif CFG.scheduler=='CosineAnnealingLR':\n","            scheduler = CosineAnnealingLR(optimizer, \n","                                          T_max      = CFG.T_max, \n","                                          eta_min    = CFG.min_lr, \n","                                          last_epoch = -1)\n","        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n","            scheduler = CosineAnnealingWarmRestarts(optimizer, \n","                                                    T_0        = CFG.T_0, \n","                                                    T_mult     = 1, \n","                                                    eta_min    = CFG.min_lr, \n","                                                    last_epoch = -1)\n","        return scheduler\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","\n","#    states = torch.load(CFG.prev_model,  map_location=torch.device('cpu'))\n","\n","    encoder = Encoder(CFG.model_name, \n","                      pretrained = True)\n","#    encoder.load_state_dict(states['encoder'])\n","    gc.collect()\n","    \n","    encoder.to(device)\n","    encoder_optimizer = Adam(encoder.parameters(), \n","                             lr           = CFG.encoder_lr, \n","                             weight_decay = CFG.weight_decay, \n","                             amsgrad      = False)\n","#    encoder_optimizer.load_state_dict(states['encoder_optimizer'])\n","    encoder_scheduler = get_scheduler(encoder_optimizer)\n","#    encoder_scheduler.load_state_dict(states['encoder_scheduler'])\n","    \n","    decoder = DecoderWithAttention(attention_dim = CFG.attention_dim, \n","                                   embed_dim     = CFG.embed_dim, \n","                                   encoder_dim   = CFG.enc_size,\n","                                   decoder_dim   = CFG.decoder_dim,\n","                                   num_layers    = CFG.decoder_layers,\n","                                   vocab_size    = len(tokenizer), \n","                                   dropout       = CFG.dropout, \n","                                   device        = device)\n","#    decoder.load_state_dict(states['decoder'])\n","    decoder.to(device)\n","    decoder_optimizer = Adam(decoder.parameters(), \n","                             lr           = CFG.decoder_lr, \n","                             weight_decay = CFG.weight_decay, \n","                             amsgrad      = False)\n","#    decoder_optimizer.load_state_dict(states['decoder_optimizer'])\n","\n","    decoder_scheduler = get_scheduler(decoder_optimizer)\n"," #   decoder_scheduler.load_state_dict(states['decoder_scheduler'])\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.stoi[\"<pad>\"])\n","\n","    best_score = np.inf\n","    best_loss  = np.inf\n","    \n","    for epoch in range(CFG.epochs):\n","        \n","        start_time = time.time()\n","        \n","        # train\n","        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n","                            encoder_optimizer, decoder_optimizer, epoch, \n","                            encoder_scheduler, decoder_scheduler, device)\n","\n","        # eval\n","        text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n","        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n","        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n","        LOGGER.info(f\"preds: {text_preds[:5]}\")\n","        \n","        # scoring\n","        score = get_score(valid_labels, text_preds)\n","        \n","        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n","            encoder_scheduler.step(score)\n","        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n","            encoder_scheduler.step()\n","        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n","            encoder_scheduler.step()\n","            \n","        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n","            decoder_scheduler.step(score)\n","        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n","            decoder_scheduler.step()\n","        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n","            decoder_scheduler.step()\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n","        \n","        if score < best_score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'encoder': encoder.state_dict(), \n","                        'encoder_optimizer': encoder_optimizer.state_dict(), \n","                        'encoder_scheduler': encoder_scheduler.state_dict(), \n","                        'decoder': decoder.state_dict(), \n","                        'decoder_optimizer': decoder_optimizer.state_dict(), \n","                        'decoder_scheduler': decoder_scheduler.state_dict(), \n","                        'text_preds': text_preds,\n","                       },\n","                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.020522,"end_time":"2021-04-07T08:27:11.156788","exception":false,"start_time":"2021-04-07T08:27:11.136266","status":"completed"},"tags":[],"id":"cCyJ-q7w9ZUW","executionInfo":{"status":"ok","timestamp":1621340269891,"user_tz":-120,"elapsed":2703,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["def get_train_file_path(image_id):\n","    return \"{}/{}/{}/{}.png\".format(\n","        image_id[0], image_id[1], image_id[2], image_id \n","    )\n","    # return CFG.train_path + \"{}/{}/{}/{}.png\".format(\n","    #     image_id[0], image_id[1], image_id[2], image_id \n","    # )"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.022257,"end_time":"2021-04-07T08:27:11.192091","exception":false,"start_time":"2021-04-07T08:27:11.169834","status":"completed"},"tags":[],"id":"9-_w62e79ZUW","executionInfo":{"status":"ok","timestamp":1621340269894,"user_tz":-120,"elapsed":2680,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["# transformations\n","\n","def get_transforms(*, data):\n","    \n","    if data == 'train':\n","        return Compose([\n","            Resize(CFG.size, CFG.size),\n","            HorizontalFlip(p=0.5),                  \n","            Transpose(p=0.5),\n","            HorizontalFlip(p=0.5),\n","            VerticalFlip(p=0.5),\n","            ShiftScaleRotate(p=0.5),   \n","            Normalize(\n","                mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225],\n","            ),\n","            ToTensorV2(),\n","        ])\n","    \n","    elif data == 'valid':\n","        return Compose([\n","            Resize(CFG.size, CFG.size),\n","            Normalize(\n","                mean=[0.485, 0.456, 0.406],\n","                std=[0.229, 0.224, 0.225],\n","            ),\n","            ToTensorV2(),\n","        ])\n"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"papermill":{"duration":0.024936,"end_time":"2021-04-07T08:27:10.869493","exception":false,"start_time":"2021-04-07T08:27:10.844557","status":"completed"},"tags":[],"id":"5OFWjf0J9ZUS","executionInfo":{"status":"ok","timestamp":1621340269895,"user_tz":-120,"elapsed":2660,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","from time import sleep\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, df, tokenizer, transform=None):\n","        super().__init__()\n","        self.df         = df\n","        self.tokenizer  = tokenizer\n","        self.file_paths = df['file_path'].values\n","        self.labels     = df['InChI_text'].values\n","        self.transform  = transform\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        # try:\n","        #, end=' ')\n","        file_path = self.file_paths[idx]\n","        # print(file_path)\n","        # image = cv2.imread(file_path)\n","        image = get_image(trainzip, file_path)\n","        image = erode_image(image)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","\n","        if self.transform:\n","            augmented = self.transform(image = image)\n","            image     = augmented['image']\n","        label = self.labels[idx]\n","        label = self.tokenizer.text_to_sequence(label)\n","        label_length = len(label)\n","        label_length = torch.LongTensor([label_length])\n","        # print('success')#, end=' ')\n","        return image, torch.LongTensor(label), label_length\n","        # except:\n","        #     return self.__getitem__(random.choice(self.file_paths))\n","    \n","\n","class TestDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        super().__init__()\n","        self.df = df\n","        self.file_paths = df['file_path'].values\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        try:\n","            file_path = self.file_paths[idx]\n","            mage = get_image(testzip, file_path)\n","            image = erode_image(image)\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image)\n","                image = augmented['image']\n","            return image\n","        except:\n","            return self.__getitem__(random.choice(self.file_paths))"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.013404,"end_time":"2021-04-07T08:27:11.218463","exception":false,"start_time":"2021-04-07T08:27:11.205059","status":"completed"},"tags":[],"id":"OaRCj9yF9ZUW"},"source":["# Data"]},{"cell_type":"code","metadata":{"papermill":{"duration":12.663809,"end_time":"2021-04-07T08:27:23.895404","exception":false,"start_time":"2021-04-07T08:27:11.231595","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"K0XQYbxS9ZUX","executionInfo":{"status":"ok","timestamp":1621340299534,"user_tz":-120,"elapsed":32279,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}},"outputId":"19ed23f4-a9ed-4602-bfb3-8dfe6a35d3a5"},"source":["train = pd.read_pickle(CFG.prep_path + 'train2.pkl')\n","\n","train['file_path'] = train['image_id'].apply(get_train_file_path)\n","\n","print(f'train.shape: {train.shape}')\n","\n","\n","if CFG.debug:\n","    CFG.epochs = 1\n","    train = train.sample(n = CFG.samp_size, random_state = CFG.seed).reset_index(drop = True)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["train.shape: (2424186, 6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"papermill":{"duration":0.02079,"end_time":"2021-04-07T08:27:23.931533","exception":false,"start_time":"2021-04-07T08:27:23.910743","status":"completed"},"tags":[],"id":"38jP1DM29ZUX","executionInfo":{"status":"ok","timestamp":1621340299544,"user_tz":-120,"elapsed":32256,"user":{"displayName":"Вася Вася","photoUrl":"","userId":"14354405213523851375"}}},"source":["train_dataset = TrainDataset(train, tokenizer, transform = get_transforms(data='train'))\n","\n","folds = train.copy()\n","Fold = StratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.seed)\n","for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n","    folds.loc[val_index, 'fold'] = int(n)\n","folds['fold'] = folds['fold'].astype(int)"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.013921,"end_time":"2021-04-07T08:27:24.004031","exception":false,"start_time":"2021-04-07T08:27:23.99011","status":"completed"},"tags":[],"id":"NzLuHJJk9ZUY"},"source":["# Training"]},{"cell_type":"code","metadata":{"papermill":{"duration":39.171513,"end_time":"2021-04-07T08:28:03.18932","exception":false,"start_time":"2021-04-07T08:27:24.017807","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"fZv3SXoH9ZUY","outputId":"d9698e20-702d-4745-f3b1-e7417900cae6"},"source":["torch.cuda.empty_cache()\n","train_loop(folds, CFG.trn_fold)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["========== fold: 0 training ==========\n","========== fold: 0 training ==========\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [1][0/10000] Data 0.980 (0.980) Elapsed 0m 2s (remain 408m 55s) Loss: 5.3236(5.3236) Encoder Grad: 2.7244  Decoder Grad: 1.0253  \n","Epoch: [1][250/10000] Data 0.066 (0.284) Elapsed 5m 7s (remain 198m 54s) Loss: 1.8375(2.5163) Encoder Grad: 0.3249  Decoder Grad: 0.5623  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OmxBFE2CZy2U"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LZOwEqDGSV_"},"source":["get_image(trainzip, 'a/8/1/a811b5b0c0a7.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBWq21jxFkPs"},"source":["<p fontsize=6>Epoch: [1][0/10000] Data 0.090 (0.090) Elapsed 0m 3s (remain 517m 6s) Loss: 5.3235(5.3235) Encoder Grad: 1.9473  Decoder Grad: 1.1028<br>\n","Epoch: [1][250/10000] Data 0.048 (0.049) Elapsed 4m 12s (remain 163m 13s) Loss: 1.8609(2.6003) Encoder Grad: 0.5123  Decoder Grad: 0.5170  <br>\n","Epoch: [1][500/10000] Data 0.040 (0.048) Elapsed 8m 18s (remain 157m 26s) Loss: 1.6802(2.1744) Encoder Grad: 0.6029  Decoder Grad: 0.6357  <br>\n","Epoch: [1][750/10000] Data 0.045 (0.048) Elapsed 12m 27s (remain 153m 22s) Loss: 1.5762(1.9931) Encoder Grad: 0.4860  Decoder Grad: 0.5464  <br>\n","Epoch: [1][1000/10000] Data 0.044 (0.047) Elapsed 16m 36s (remain 149m 15s) Loss: 1.6858(1.8833) Encoder Grad: 0.4295  Decoder Grad: 0.6595<br>  \n","Epoch: [1][1250/10000] Data 0.036 (0.046) Elapsed 20m 42s (remain 144m 51s) Loss: 1.3590(1.8070) Encoder Grad: 0.3283  Decoder Grad: 0.4243  <br>\n","Epoch: [1][1500/10000] Data 0.042 (0.046) Elapsed 24m 51s (remain 140m 42s) Loss: 1.4146(1.7492) Encoder Grad: 0.3183  Decoder Grad: 0.4918  <br>\n","Epoch: [1][1750/10000] Data 0.073 (0.045) Elapsed 28m 59s (remain 136m 34s) Loss: 1.4380(1.7012) Encoder Grad: 0.5455  Decoder Grad: 0.5015  <br>\n","Epoch: [1][2000/10000] Data 0.031 (0.045) Elapsed 33m 9s (remain 132m 34s) Loss: 1.3016(1.6623) Encoder Grad: 0.6707  Decoder Grad: 0.8680  <br>\n","Epoch: [1][2250/10000] Data 0.037 (0.046) Elapsed 37m 21s (remain 128m 37s) Loss: 1.2554(1.6272) Encoder Grad: 0.5491  Decoder Grad: 0.5673  <br>\n","Epoch: [1][2500/10000] Data 0.038 (0.046) Elapsed 41m 30s (remain 124m 26s) Loss: 1.2604(1.5955) Encoder Grad: 0.7967  Decoder Grad: 1.0343"]}]}